{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850dfe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\anaconda3\\envs\\testing_clone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70186a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image / 127.5) - 1\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaec13e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 1728        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36864       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 73728       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147456      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization (Instanc (None, 64, 64, 128)  256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 256)  294912      instance_normalization[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  589824      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_1 (Insta (None, 32, 32, 256)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 512)  1179648     instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  2359296     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_2 (Insta (None, 16, 16, 512)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 1024)   4718592     instance_normalization_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   9437184     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 512)  2097152     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1024) 0           instance_normalization_2[0][0]   \n",
      "                                                                 conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  4718592     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 16, 512)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 256)  524288      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 512)  0           instance_normalization_1[0][0]   \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  1179648     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  589824      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 256)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 128)  131072      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 256)  0           instance_normalization[0][0]     \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  294912      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147456      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 64) 32768       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 73728       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 36864       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 3)  1731        conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,028,867\n",
      "Trainable params: 31,028,867\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from custom import custom_unet_generator_v2,custom_unet_descriminator_dilated_v2,custom_unet_descriminator_v2\n",
    "generator_g = custom_unet_generator_v2()\n",
    "generator_f = custom_unet_generator_v2()\n",
    "print(generator_g.summary())\n",
    "\n",
    "discriminator_x = custom_unet_descriminator_v2(target=False)\n",
    "discriminator_y = custom_unet_descriminator_v2(target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4262d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "  \n",
    "  return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cceaa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/best/ckpt-9\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = \"./checkpoints/train/\"\n",
    "checkpoint_path = \"./checkpoints/best/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   ckpt.restore(ckpt_manager.checkpoints[8]) #best checkpoint\n",
    "#   print ('Latest checkpoint restored!!')\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#   print ('Latest checkpoint restored!!')\n",
    "  print(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef943f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ckpt_manager.checkpoints[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07da82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_eval(model, test_input):\n",
    "  prediction = model(test_input)\n",
    "    \n",
    "  plt.figure(figsize=(12, 12))\n",
    "\n",
    "  display_list = [test_input[0], prediction[0]]\n",
    "  title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "  for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4b67cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra', with_info=True, as_supervised=True)\n",
    "test_horses = dataset['testA']\n",
    "\n",
    "test_horses_np = []\n",
    "\n",
    "for image_tuple in test_horses:\n",
    "    image, _ = image_tuple  # unpack the tuple into individual elements\n",
    "    image_np = image.numpy()\n",
    "    test_horses_np.append(image_np)\n",
    "\n",
    "print(len(test_horses_np))  # output: number_of_samples\n",
    "print(test_horses_np[0].shape)  # output: (height, width, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "ind=0\n",
    "cyclic_loss = 0\n",
    "for input_image in test_horses_np:\n",
    "    \n",
    "    input_image = normalize(input_image)\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    reconstructed_image = generate_images_eval(generator_g, input_image)\n",
    "    cycled_input_image = generator_f(reconstructed_image, training=False)\n",
    "    cyclic_loss = cyclic_loss + calc_cycle_loss(input_image, cycled_input_image).numpy()\n",
    "    #below code is to save real image,generated image and cycled image\n",
    "#     save1 = np.array(reconstructed_image)\n",
    "#     save1 = (save1 + 1) * 127.5\n",
    "#     save1 = np.squeeze(save1, axis=0)\n",
    "#     save1 = array_to_img(save1)\n",
    "    \n",
    "#     ori = np.array(input_image)\n",
    "#     ori = (ori + 1) * 127.5\n",
    "#     ori = np.squeeze(ori, axis=0)\n",
    "#     ori = array_to_img(ori)\n",
    "    \n",
    "\n",
    "#     cycled_input_image = generator_f(reconstructed_image, training=False)\n",
    "    \n",
    "#     save2 = np.array(cycled_input_image)\n",
    "#     save2 = (save2 + 1) * 127.5\n",
    "#     save2 = np.squeeze(save2, axis=0)\n",
    "#     save2 = array_to_img(save2)\n",
    "    \n",
    "#     stack = np.hstack((ori,save1))\n",
    "#     stack = np.hstack((stack,save2))\n",
    "#     stack = cv2.cvtColor(stack,cv2.COLOR_BGR2RGB)\n",
    "#     name=str(ind)+\".png\"\n",
    "#     cv2.imwrite(\"E:/computer vision/project/changes/submisssion/data/model3/horse_cycledhorse/\"+name, np.array(stack))\n",
    "#     ind=ind+1\n",
    "\n",
    "print(\"cyclic_loss=\",cyclic_loss/len(test_horses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a550ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35835b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6faf447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra', with_info=True, as_supervised=True)\n",
    "\n",
    "test_zebras = dataset['testB'] #loads zebras\n",
    "\n",
    "test_zebras_np = []\n",
    "\n",
    "for image_tuple in test_zebras:\n",
    "    image, _ = image_tuple  # unpack the tuple into individual elements\n",
    "    image_np = image.numpy()\n",
    "    test_zebras_np.append(image_np)\n",
    "\n",
    "print(len(test_zebras))  # output: number_of_samples\n",
    "print(test_horses_np[0].shape)  # output: (height, width, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "cyclic_loss = 0\n",
    "ind=0\n",
    "for input_image in test_zebras_np:\n",
    "    \n",
    "    input_image = normalize(input_image)\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    reconstructed_image = generate_images_eval(generator_f, input_image)\n",
    "    cycled_input_image = generator_g(reconstructed_image, training=False)\n",
    "    cyclic_loss = cyclic_loss + calc_cycle_loss(input_image, cycled_input_image).numpy()\n",
    "#     below codes saves original image, generated image and cycled image\n",
    "#     save1 = np.array(reconstructed_image)\n",
    "#     save1 = (save1 + 1) * 127.5\n",
    "#     save1 = np.squeeze(save1, axis=0)\n",
    "#     save1 = array_to_img(save1)\n",
    "    \n",
    "#     ori = np.array(input_image)\n",
    "#     ori = (ori + 1) * 127.5\n",
    "#     ori = np.squeeze(ori, axis=0)\n",
    "#     ori = array_to_img(ori)\n",
    "    \n",
    "    \n",
    "#     save2 = np.array(cycled_input_image)\n",
    "#     save2 = (save2 + 1) * 127.5\n",
    "#     save2 = np.squeeze(save2, axis=0)\n",
    "#     save2 = array_to_img(save2)\n",
    "    \n",
    "#     stack = np.hstack((ori,save1))\n",
    "#     stack = np.hstack((stack,save2))\n",
    "#     stack = cv2.cvtColor(stack,cv2.COLOR_BGR2RGB)\n",
    "#     name=str(ind)+\".png\"\n",
    "#     cv2.imwrite(\"E:/computer vision/project/changes/submisssion/data/model3/zebra_cycledzebra/\"+name, np.array(stack))\n",
    "#     ind=ind+1\n",
    "\n",
    "print(\"cyclic_loss=\",cyclic_loss/len(test_horses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bb4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76c203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e27b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233241e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582b582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bd197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00d030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d385e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e74201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c528263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3504fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb50f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8f447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fd078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b99f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aaec85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1f038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774c44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177420f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4763c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c7e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2faea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8387869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fb4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fce50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b3ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570eaf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa7dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
